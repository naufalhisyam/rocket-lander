{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install python-opengl\n",
    "!apt install ffmpeg\n",
    "!apt install xvfb\n",
    "\n",
    "!pip install cvxpy\n",
    "!pip install box2d-py\n",
    "!pip uninstall pyglet -y\n",
    "!pip uninstall gym -y\n",
    "!pip install pyglet==1.3.2\n",
    "!pip install gym==0.9.4\n",
    "!pip install pyvirtualdisplay\n",
    "!pip uninstall tensorflow\n",
    "!pip install tensorflow-gpu==1.15\n",
    "!apt install --allow-change-held-packages libcudnn7=7.4.1.5-1+cuda10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b evo-alg https://github.com/naufalhisyam/rocket-lander.git\n",
    "%cd /content/rocket-lander\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1000, 800))\n",
    "virtual_display.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "%cd /content/rocket-lander\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy.core.numeric import False_\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from control_and_ai.DDPG.ddpg import DDPG\n",
    "from control_and_ai.DDPG.utils import Utils\n",
    "from control_and_ai.DDPG.exploration import OUPolicy\n",
    "\n",
    "from constants import *\n",
    "from constants import DEGTORAD\n",
    "from environments.rocketlander_test import RocketLander, get_state_sample\n",
    "\n",
    "action_bounds = [1, 1, 15*DEGTORAD]\n",
    "\n",
    "eps = []\n",
    "eps.append(OUPolicy(0, 0.2, 0.4))\n",
    "eps.append(OUPolicy(0, 0.2, 0.4))\n",
    "eps.append(OUPolicy(0, 0.2, 0.4))\n",
    "\n",
    "simulation_settings = {'Side Engines': True,\n",
    "                       'Clouds': True,\n",
    "                       'Vectorized Nozzle': True,\n",
    "                       'Graph': False,\n",
    "                       'Render': False,\n",
    "                       'Starting Y-Pos Constant': 1,\n",
    "                       'Initial Force': 'random',\n",
    "                       'Rows': 1,\n",
    "                       'Columns': 2,\n",
    "                       'Episodes': 500}\n",
    "\n",
    "env = RocketLander(simulation_settings)\n",
    "#env = wrappers.Monitor(env, './rocket_videos', force=True)\n",
    "\n",
    "#Set both line below to False if you want to contniue training from a saved checkpoint\n",
    "RETRAIN = False#Restore weights if False\n",
    "TEST = True #Test the model\n",
    "\n",
    "NUM_EPISODES = 1\n",
    "SAVE_REWARD = True\n",
    "SAVE_TO_EXCEL = True #Export states & actions logs as .xlsx\n",
    "\n",
    "NAME = \"test\" #Model name\n",
    "\n",
    "SIMULATE_WIND = False\n",
    "x_force = 0 # x-axis wind force in Newton\n",
    "y_force = 0 # y-axis wind force in Newton\n",
    "\n",
    "model_dir = '/content/gdrive/MyDrive/colab_model/rocket/DDPG/' + NAME\n",
    "\n",
    "agent = DDPG(\n",
    "    action_bounds,\n",
    "    eps,\n",
    "    env.observation_space.shape[0], #for first model\n",
    "    actor_learning_rate=0.001,\n",
    "    critic_learning_rate=0.01,\n",
    "    retrain=RETRAIN,\n",
    "    log_dir=\"./logs\",\n",
    "    model_dir=model_dir,\n",
    "    batch_size=100,\n",
    "    gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, agent, x_force, y_force):\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "\n",
    "    util = Utils()\n",
    "    state_samples = get_state_sample(samples=5000, normal_state=True)\n",
    "    util.create_normalizer(state_sample=state_samples)\n",
    "    xpos, ypos, xvel, yvel, lander_angle, angular_vel, rem_fuel, lander_mass, xpos_rocket, ypos_rocket, xpos_landingPad, ypos_landingPad = ([] for _ in range(12))\n",
    "    fE, fS, pSi = ([] for _ in range(3))\n",
    "    \n",
    "    if SAVE_REWARD:\n",
    "        rew, total_rew = [], []\n",
    "        shaping_pos, shaping_vel = [], []\n",
    "        shaping_ang, shaping_angvel = [], []\n",
    "        shaping_leftleg, shaping_rightleg = [], []\n",
    "        shaping_fe, shaping_fs = [], []\n",
    "        ep = []\n",
    "    \n",
    "    if SIMULATE_WIND:\n",
    "        wind_x, wind_y = ([] for _ in range(2))\n",
    "    \n",
    "    for episode in range(1, NUM_EPISODES + 1):\n",
    "        old_state = None\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        state = env.reset()\n",
    "        state = util.normalize(state)\n",
    "        max_steps = 500\n",
    "\n",
    "        left_or_right_barge_movement = np.random.randint(0, 2)\n",
    "\n",
    "        for t in range(max_steps): # env.spec.max_episode_steps\n",
    "            old_state = state\n",
    "            # infer an action\n",
    "            action = agent.get_action(np.reshape(state, (1, obs_size)), not TEST)\n",
    "            \n",
    "            current_state = env.get_state_with_barge_and_landing_coordinates(untransformed_state=True)\n",
    "                \n",
    "            xpos.append(current_state[0]-current_state[12]) #xpos_rocket - xpos_landingPad\n",
    "            ypos.append(current_state[1]-current_state[13]) #ypos_rocket - ypos_landingPad\n",
    "            xvel.append(current_state[2]) #xdot\n",
    "            yvel.append(current_state[3]) #ydot\n",
    "            lander_angle.append(current_state[4]) #theta\n",
    "            angular_vel.append(current_state[5]) #theta_dot\n",
    "            rem_fuel.append(current_state[6]) #initial fuel = 0.2 * initial_mass\n",
    "            lander_mass.append(current_state[7]) #initial_mass = 25.222\n",
    "            xpos_rocket.append(current_state[0]) # xpos_rocket\n",
    "            ypos_rocket.append(current_state[1]) # ypos_rocket\n",
    "            xpos_landingPad.append(current_state[12]) # xpos_landingPad\n",
    "            ypos_landingPad.append(current_state[13]) # ypos_landingPad\n",
    "            fE.append(action[0][0])\n",
    "            fS.append(action[0][1])\n",
    "            pSi.append(action[0][2])\n",
    "\n",
    "            # take it\n",
    "            state, reward, shaping_element, done, _ = env.step(action[0])\n",
    "            state = util.normalize(state)\n",
    "            total_reward += reward\n",
    "            rew.append(reward)\n",
    "            shaping_pos.append(shaping_element[0])\n",
    "            shaping_vel.append(shaping_element[1])\n",
    "            shaping_ang.append(shaping_element[2])\n",
    "            shaping_angvel.append(shaping_element[3])\n",
    "            shaping_leftleg.append(shaping_element[4])\n",
    "            shaping_rightleg.append(shaping_element[5])\n",
    "            shaping_fe.append(shaping_element[6])\n",
    "            shaping_fs.append(shaping_element[7])\n",
    "\n",
    "            if SIMULATE_WIND:\n",
    "                if state[LEFT_GROUND_CONTACT] == 0 and state[RIGHT_GROUND_CONTACT] == 0:\n",
    "                    env.apply_random_x_disturbance(epsilon=0.005, left_or_right=left_or_right_barge_movement, x_force=x_force)\n",
    "                    env.apply_random_y_disturbance(epsilon=0.005, y_force=y_force)\n",
    "                    if SAVE_TO_EXCEL:\n",
    "                        winds = env.get_winds_value()\n",
    "                        wind_x.append(winds[0])\n",
    "                        wind_y.append(winds[1])\n",
    "            \n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # agent.log_data(total_reward, episode)\n",
    "\n",
    "        print(\"Episode:\\t{0}\\tReward:\\t{1}\".format(episode, total_reward))\n",
    "    \n",
    "        if SAVE_REWARD:\n",
    "            total_rew.append(total_reward)\n",
    "            ep.append(episode)\n",
    "\n",
    "    if SAVE_REWARD:\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        tot_reward=pd.DataFrame(list(zip(ep,total_rew)),\n",
    "                                 columns=['episode','total_reward'])\n",
    "        with pd.ExcelWriter(model_dir + f\"/DDPG_test-eps-rewards_{NAME}_{total_reward}_{len(ep)}.xlsx\") as writer:\n",
    "            tot_reward.to_excel(writer, sheet_name=f\"{NAME}_test-eps-rewards\")\n",
    "\n",
    "        reward_data=pd.DataFrame(list(zip(rew,\n",
    "                                          shaping_pos,\n",
    "                                          shaping_vel,\n",
    "                                          shaping_ang,\n",
    "                                          shaping_angvel,\n",
    "                                          shaping_leftleg,\n",
    "                                          shaping_rightleg,\n",
    "                                          shaping_fe,\n",
    "                                          shaping_fs)),\n",
    "                                 columns=['reward',\n",
    "                                          'shaping_pos',\n",
    "                                          'shaping_vel',\n",
    "                                          'shaping_ang',\n",
    "                                          'shaping_angvel',\n",
    "                                          'shaping_leftleg',\n",
    "                                          'shaping_rightleg',\n",
    "                                          'r_fe',\n",
    "                                          'r_fs'])\n",
    "        with pd.ExcelWriter(model_dir + f\"/DDPG_test-reward-element_{NAME}_{total_reward}_{len(ep)}.xlsx\") as writer:\n",
    "            reward_data.to_excel(writer, sheet_name=f\"{NAME}_test-eps-rewards\")\n",
    "\n",
    "    if SAVE_TO_EXCEL:\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        state_data=pd.DataFrame(list(zip(xpos,ypos,xvel,yvel,lander_angle,angular_vel,rem_fuel,lander_mass,xpos_rocket,ypos_rocket,xpos_landingPad,ypos_landingPad)),\\\n",
    "            columns=['x_pos','y_pos','x_vel','y_vel','lateral_angle','angular_velocity','remaining_fuel','lander_mass','xpos_rocket','ypos_rocket','xpos_landingPad','ypos_landingPad'])\n",
    "        action_data=pd.DataFrame(list(zip(fE,fS,pSi)),columns=['Fe','Fs','Psi'])\n",
    "        if SIMULATE_WIND:\n",
    "            wind_dat= pd.DataFrame(list(zip(wind_x, wind_y)),columns=['x_wind force', 'y_wind force'])\n",
    "                \n",
    "        with pd.ExcelWriter(model_dir + f\"/DDPG_{NAME}_{total_reward}.xlsx\") as writer:\n",
    "            state_data.to_excel(writer, sheet_name=\"state\")\n",
    "            action_data.to_excel(writer, sheet_name=\"action\")\n",
    "            if SIMULATE_WIND:\n",
    "                wind_dat.to_excel(writer, sheet_name=\"winds\")    \n",
    "        print('Steps:', len(state_data))\n",
    "        print(\"Logs saved\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_ylim(0,21)\n",
    "    ax.set_xlim(-10,10)\n",
    "    ax.plot(xpos, ypos, color='maroon')\n",
    "    ax.axvline(x=0, c='gray', linewidth=0.7)\n",
    "    ax.set_title('Rocket landing trajectory')\n",
    "    ax.set_xlabel('x position (m)')\n",
    "    ax.set_ylabel('y position (m)')\n",
    "    ax.grid()\n",
    "    print(f\"X miss distance : {xpos[-1]}\\nY miss distance : {ypos[-1]}\")\n",
    "    print(f\"last X velocity : {xvel[-1]}\\nlast Y velocity : {yvel[-1]}\")\n",
    "    print(f\"last angle : {lander_angle[-1]}\\nlast angular velocity : {angular_vel[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(env, agent, x_force, y_force)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
