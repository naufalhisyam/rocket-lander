{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<LARGE>Set runtime type to 'None'</LARGE>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/gdrive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Installing required libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!apt install python-opengl\n",
                "!apt install ffmpeg\n",
                "!apt install xvfb\n",
                "\n",
                "!pip install cvxpy\n",
                "!pip install box2d-py\n",
                "!pip uninstall pyglet -y\n",
                "!pip uninstall gym -y\n",
                "!pip install pyglet==1.3.2\n",
                "!pip install gym==0.9.4\n",
                "!pip install pyvirtualdisplay\n",
                "!pip uninstall tensorflow\n",
                "!pip install tensorflow-gpu==1.15\n",
                "!apt install --allow-change-held-packages libcudnn7=7.4.1.5-1+cuda10.0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!git clone -b td3 https://github.com/naufalhisyam/rocket-lander.git\n",
                "%cd /content/rocket-lander\n",
                "!ls"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyvirtualdisplay import Display\n",
                "\n",
                "display = Display(visible=0, size=(1000, 800))\n",
                "display.start()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%tensorflow_version 1.x\n",
                "%cd /content/rocket-lander\n",
                "\n",
                "import os\n",
                "import numpy as np\n",
                "from numpy.core.numeric import False_\n",
                "import pandas as pd\n",
                "import tensorflow as tf\n",
                "\n",
                "from control_and_ai.DDPG.ddpg import DDPG\n",
                "from control_and_ai.DDPG.utils import Utils\n",
                "from control_and_ai.DDPG.exploration import OUPolicy\n",
                "\n",
                "from constants import *\n",
                "from constants import DEGTORAD\n",
                "from environments.rocketlander import RocketLander, get_state_sample\n",
                "\n",
                "os.makedirs(\"/content/gdrive/MyDrive/colab_model/rocket/DDPG/\", exist_ok=True)\n",
                "action_bounds = [1, 1, 15*DEGTORAD]\n",
                "\n",
                "eps = []\n",
                "eps.append(OUPolicy(0, 0.2, 0.4))\n",
                "eps.append(OUPolicy(0, 0.2, 0.4))\n",
                "eps.append(OUPolicy(0, 0.2, 0.4))\n",
                "\n",
                "simulation_settings = {'Side Engines': True,\n",
                "                       'Clouds': True,\n",
                "                       'Vectorized Nozzle': False,\n",
                "                       'Graph': False,\n",
                "                       'Render': False,\n",
                "                       'Starting Y-Pos Constant': 1,\n",
                "                       'Initial Force': 'random',\n",
                "                       'Rows': 1,\n",
                "                       'Columns': 2,\n",
                "                       'Episodes': 500}\n",
                "env = RocketLander(simulation_settings)\n",
                "\n",
                "#Set both line below to False if you want to contniue training from a saved checkpoint\n",
                "RETRAIN = True #Restore weights if False\n",
                "TEST = False #Test the model\n",
                "\n",
                "NUM_EPISODES = 500\n",
                "SAVE_REWARD = True #Export reward log as .xlsx\n",
                "NAME = \"test\" #Model name\n",
                "\n",
                "model_dir = '/content/gdrive/MyDrive/colab_model/rocket/DDPG/' + NAME\n",
                "\n",
                "with tf.device('/cpu:0'):\n",
                "    agent = DDPG(\n",
                "        action_bounds,\n",
                "        eps,\n",
                "        env.observation_space.shape[0], #for first model\n",
                "        actor_learning_rate=0.001,\n",
                "        critic_learning_rate=0.01,\n",
                "        retrain=RETRAIN,\n",
                "        log_dir=\"./logs\",\n",
                "        model_dir=model_dir,\n",
                "        batch_size=100,\n",
                "        gamma=0.99)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(env, agent):\n",
                "    obs_size = env.observation_space.shape[0]\n",
                "\n",
                "    util = Utils()\n",
                "    state_samples = get_state_sample(samples=5000, normal_state=True)\n",
                "    util.create_normalizer(state_sample=state_samples)\n",
                "    if SAVE_REWARD:\n",
                "        rew = []\n",
                "        ep = []\n",
                "\n",
                "    for episode in range(1, NUM_EPISODES + 1):\n",
                "        old_state = None\n",
                "        done = False\n",
                "        total_reward = 0\n",
                "\n",
                "        state = env.reset()\n",
                "        state = util.normalize(state)\n",
                "        max_steps = 500\n",
                "\n",
                "        left_or_right_barge_movement = np.random.randint(0, 2)\n",
                "\n",
                "        for t in range(max_steps): # env.spec.max_episode_steps\n",
                "            old_state = state\n",
                "            # infer an action\n",
                "            action = agent.get_action(np.reshape(state, (1, obs_size)), not TEST)\n",
                "\n",
                "            # take it\n",
                "            state, reward, done, _ = env.step(action[0])\n",
                "            state = util.normalize(state)\n",
                "            total_reward += reward\n",
                "\n",
                "            if state[LEFT_GROUND_CONTACT] == 0 and state[RIGHT_GROUND_CONTACT] == 0:\n",
                "                #env.move_barge_randomly(epsilon, left_or_right_barge_movement)\n",
                "                env.apply_random_x_disturbance(epsilon=0.005, left_or_right=left_or_right_barge_movement)\n",
                "                env.apply_random_y_disturbance(epsilon=0.005)\n",
                "\n",
                "            if not TEST:\n",
                "                # update q vals\n",
                "                agent.update(old_state, action[0], np.array(reward), state, done)\n",
                "\n",
                "            if done:\n",
                "                break\n",
                "\n",
                "        agent.log_data(total_reward, episode)\n",
                "\n",
                "        if episode % 50 == 0 and not TEST:\n",
                "            print('Saved model at episode', episode)\n",
                "            agent.save_model(episode)\n",
                "        if SAVE_REWARD:\n",
                "            rew.append(total_reward)\n",
                "            ep.append(episode)\n",
                "        print(\"Episode:\\t{0}\\tReward:\\t{1}\".format(episode, total_reward))\n",
                "    \n",
                "    if SAVE_REWARD:\n",
                "        os.makedirs(\"excel_logs/eps-rewards/\", exist_ok=True)\n",
                "        reward_data=pd.DataFrame(list(zip(ep,rew)),columns=['episode','reward'])\n",
                "        with pd.ExcelWriter(f\"/content/rocket-lander/excel_logs/eps-rewards/DDPG_eps-rewards_{NAME}_{rew[-1]}_{len(ep)}.xlsx\") as writer:\n",
                "            reward_data.to_excel(writer, sheet_name=f\"{NAME}_eps-rewards\")\n",
                "        !cp -a \"/content/rocket-lander/excel_logs/eps-rewards/.\" \"{model_dir}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with tf.device('/cpu:0'):\n",
                "    train(env, agent)"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
